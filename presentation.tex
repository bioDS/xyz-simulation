\documentclass[8pt]{beamer}
\usetheme{default}

\usepackage{todonotes}
\presetkeys{todonotes}{inline}{}

%\geometry{paperwidth=140mm,paperheight=105mm}

\title{Inferring genetic interactions using xyz}
\author{Kieran Elmes}
\begin{document}
\begin{frame}[plain]
    \maketitle
\end{frame}
\begin{frame}{What are we doing}
\todo{detail}
Simulating large(ish) sets of SiRNA knockdowns, and measuring xyz's ability to correctly determine interactions in the data.

We can already do this with glinternet, but xyz is (several hundred times?) faster.
\end{frame}
\begin{frame}{Why do we care}
\todo[inline]{everything}
Genetic interactions are useful. For reasons.
\end{frame}
\begin{frame}{$P = 100$}
Results for $p=100$ using vanilla xyz
\begin{center}
\begin{minipage}{0.9\linewidth}
	\centering
	\includegraphics[width=0.5\linewidth]{"PrecRecF1/PrecRecF1_n1000_tno-vanilla_xyz"}%
	\includegraphics[width=0.5\linewidth]{"PrecRecF1/PrecRecF1_n1000_tyes-vanilla_xyz"}
\end{minipage}
\end{center}
For 1000 measurements of only 100 genes, results are good.
\end{frame}
\begin{frame}{$P = 1000$}
Results for $p=1000$ using vanilla xyz
\begin{center}
	\begin{minipage}{0.9\linewidth}
		\centering
		\includegraphics[width=0.5\linewidth]{"PrecRecF1/PrecRecF1_n10000_tno-vanilla_xyz"}%
		\includegraphics[width=0.5\linewidth]{"PrecRecF1/PrecRecF1_n10000_tyes-vanilla_xyz"}
	\end{minipage}
\end{center}
Large data sets are a problem, however.
\end{frame}
\begin{frame}{$P = 1000$, increased limits in xyz}
Results for $p=1000$ using increased limits\footnote{commit 4517afbc59bdf091eed75683a28070476933f5da}
\begin{center}
	\begin{minipage}{0.9\linewidth}
		\centering
		\includegraphics[width=0.5\linewidth]{"PrecRecF1/increased_limits/PrecRecF1_n10000_tno"}%
		\includegraphics[width=0.5\linewidth]{"PrecRecF1/increased_limits/PrecRecF1_n10000_tyes"}
	\end{minipage}
\end{center}
Increasing the maximum allowed number of interactions in xyz does not improve results.
\end{frame}
\begin{frame}{Larger values of $L$, $SNR = 2$}
\begin{center}
	\begin{minipage}{0.9\linewidth}
		\centering
		\includegraphics[width=0.5\linewidth]{"l_diff/l_diff_n10000_SNR2_tno"}%
		\includegraphics[width=0.5\linewidth]{"l_diff/l_diff_n10000_SNR2_tyes"}
	\end{minipage}
\end{center}

\end{frame}
\begin{frame}{Larger values of $L$, $SNR = 5$}
\begin{center}
	\begin{minipage}{0.9\linewidth}
		\centering
		\includegraphics[width=0.5\linewidth]{"l_diff/l_diff_n10000_SNR5_tno"}%
		\includegraphics[width=0.5\linewidth]{"l_diff/l_diff_n10000_SNR5_tyes"}
	\end{minipage}
\end{center}

\end{frame}
\begin{frame}{Larger values of $L$, $SNR = 10$}
\begin{center}
	\begin{minipage}{0.9\linewidth}
		\centering
		\includegraphics[width=0.5\linewidth]{"l_diff/l_diff_n10000_SNR10_tno"}%
		\includegraphics[width=0.5\linewidth]{"l_diff/l_diff_n10000_SNR10_tyes"}
	\end{minipage}
\end{center}


Where results are found, a larger value of $L$ does not obviously improve precision. It does seem to improve the odds of finding at least some results, however.
\end{frame}
\begin{frame}{Number of results}
\begin{center}
	\begin{minipage}{0.5\linewidth}
		\centering
		\includegraphics[width=\linewidth]{"l_diff/quant_analysis_n10000"}
	\end{minipage}%
	\begin{minipage}{0.1\linewidth}
\ 
	\end{minipage}%
	\begin{minipage}{0.4\linewidth}
		\begin{itemize}
			\item $L = 1000$ produces a smaller set of results than $L=100$, with similar accuracy.
			\item For our purposes $L = 100$ will be used from now on.
		\end{itemize}
	\end{minipage}
\end{center}



\end{frame}
\begin{frame}{Interaction strength}
\begin{center}
	\begin{minipage}{0.9\linewidth}
		\centering
		\includegraphics[width=0.5\linewidth]{"FXstrength/FXstrength_PRF_n10000_L100_tno"}%
		\includegraphics[width=0.5\linewidth]{"FXstrength/FXstrength_PRF_n10000_L100_tyes"}
	\end{minipage}
\end{center}


Fixing $L = 100$, a small number of strong interactions are reliably found, even in a large data set. When the data contains a large number of total interactions, we find almost nothing.
\end{frame}

\begin{frame}
\todo[inline]{wording/detail}
Does xyz perform any better when there are fewer interactions in a large dataset?
\end{frame}

\end{document}

% Things to mention: (also see notes)
