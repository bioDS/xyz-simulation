\begin{filecontents*}{presentation.bib}
@article{Kaelin:2005gm,
	author = {Kaelin, William G},
	title = {{The concept of synthetic lethality in the context of anticancer therapy.}},
	journal = {Nat. Rev. Cancer},
	year = {2005}
}
\end{filecontents*}

\documentclass[8pt]{beamer}
\usetheme{Hannover}

\usepackage[style=numeric,firstinits=true,backend=biber]{biblatex}
\addbibresource{presentation.bib}
\usepackage{todonotes}

\presetkeys{todonotes}{inline}{}

%\geometry{paperwidth=140mm,paperheight=105mm}

\title{Inferring genetic interactions using xyz}
\author{Kieran Elmes}
\begin{document}
\begin{frame}[plain]
    \maketitle
\end{frame}

\begin{frame}{The Problem}
\todo[inline]{move this up?}
\begin{minipage}{0.4\linewidth}
	\includegraphics[width=0.8\linewidth]{"cells_2x"}\\[-1ex]
	{\tiny Obligatory xkcd\\Source: \href{https://www.xkcd.com/1217/}{https://www.xkcd.com/1217/}}
\end{minipage}%
\begin{minipage}{0.6\linewidth}
	\begin{itemize}
		\item Killing cancer cells is easy. Killing cancer cells selectively is hard.
		\item Synthetic lethal interactions could potentially be used for exactly that.\cite{Kaelin:2005gm}
		\item We have approx. 20,000 genes. Experimentally testing all 200 million pairwise interactions is awkward.
	\end{itemize}
\end{minipage}
\end{frame}

\begin{frame}{What are we doing}
\todo{detail, how do glinternet/xyz work?}
Simulating large(ish) sets of SiRNA knockdowns, and measuring xyz's ability to correctly determine interactions in the data.

We can already do this with glinternet, but xyz is (several hundred times?) faster.


\end{frame}
\begin{frame}{The Data}
\begin{itemize}
	\item SiRNAs can reduce the expression of several hundred genes at once.
	\item $X \leftarrow n \times p$ matrix, $n$ trials with $p$ affected genes.
	\item $Y \leftarrow p$ fitness values.
\end{itemize}


\end{frame}

\begin{frame}{$P = 100$}
Results for $p=100$ using vanilla xyz
\begin{center}
\begin{minipage}{0.9\linewidth}
	\centering
	\includegraphics[width=0.5\linewidth]{"PrecRecF1/PrecRecF1_n1000_tno-vanilla_xyz"}%
	\includegraphics[width=0.5\linewidth]{"PrecRecF1/PrecRecF1_n1000_tyes-vanilla_xyz"}
\end{minipage}
\end{center}
For 1000 measurements of only 100 genes, results are good.
\end{frame}
\begin{frame}{$P = 1000$}
Results for $p=1000$ using vanilla xyz
\begin{center}
	\begin{minipage}{0.9\linewidth}
		\centering
		\includegraphics[width=0.5\linewidth]{"PrecRecF1/PrecRecF1_n10000_tno-vanilla_xyz"}%
		\includegraphics[width=0.5\linewidth]{"PrecRecF1/PrecRecF1_n10000_tyes-vanilla_xyz"}
	\end{minipage}
\end{center}
Large data sets are a problem, however.
\end{frame}
\begin{frame}{$P = 1000$, increased limits in xyz}
Results for $p=1000$ using increased limits\footnote{commit 4517afbc59bdf091eed75683a28070476933f5da}
\begin{center}
	\begin{minipage}{0.9\linewidth}
		\centering
		\includegraphics[width=0.5\linewidth]{"PrecRecF1/increased_limits/PrecRecF1_n10000_tno"}%
		\includegraphics[width=0.5\linewidth]{"PrecRecF1/increased_limits/PrecRecF1_n10000_tyes"}
	\end{minipage}
\end{center}
Increasing the maximum allowed number of interactions in xyz does not improve results.
\end{frame}

\begin{frame}{Larger values of $L$}
\begin{center}
	\begin{minipage}{0.5\linewidth}
		\centering
		\includegraphics[width=\linewidth]{"l_diff/quant_analysis_n10000"}
	\end{minipage}%
	\begin{minipage}{0.1\linewidth}
		\ 
	\end{minipage}%
	\begin{minipage}{0.4\linewidth}
		\begin{itemize}
			\item $L = 1000$ produces a smaller set of results than $L=100$, with similar accuracy.
			\item For our purposes $L = 100$ will be used from now on.
		\end{itemize}
	\end{minipage}
\end{center}

\end{frame}
\begin{frame}{Larger values of $L$, $SNR = 5$}
\begin{center}
	\begin{minipage}{0.9\linewidth}
		\centering
		\includegraphics[width=0.5\linewidth]{"l_diff/l_diff_n10000_SNR5_tno"}%
		\includegraphics[width=0.5\linewidth]{"l_diff/l_diff_n10000_SNR5_tyes"}
	\end{minipage}
\end{center}

\end{frame}

\begin{frame}{Interaction strength}
\begin{center}
	\begin{minipage}{0.9\linewidth}
		\centering
		\includegraphics[width=0.5\linewidth]{"FXstrength/FXstrength_PRF_n10000_L100_tno"}%
		\includegraphics[width=0.5\linewidth]{"FXstrength/FXstrength_PRF_n10000_L100_tyes"}
	\end{minipage}
\end{center}


Fixing $L = 100$, a small number of strong interactions are reliably found, even in a large data set. When the data contains a large number of total interactions, we find almost nothing. If nothing else, our ability to detect synthetic lethals still looks good.
\end{frame}

\begin{frame}
\todo[inline]{wording/detail}
Does xyz perform any better when there are fewer interactions in a large dataset?
\end{frame}

\begin{frame}{Large data, few interactions}
	\begin{minipage}{0.9\linewidth}
		\centering
		\includegraphics[width=0.5\linewidth]{"PrecRecF1/PrecRecF1_n10000_tno_large0"}%
		\includegraphics[width=0.5\linewidth]{"PrecRecF1/PrecRecF1_n10000_tyes_large0"}
	\end{minipage}
\end{frame}

\begin{frame}{WIP/TODO}
\begin{itemize}
	\item Test detection of synthetic lethals specifically, including glinternet.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{References}
\printbibliography
\end{frame}
\end{document}



% Things to mention: (also see notes)
